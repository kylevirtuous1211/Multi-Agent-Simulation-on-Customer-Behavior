{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb884cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file\n",
    "current_dir = os.getcwd()\n",
    "env_path = os.path.join(current_dir, '.env')\n",
    "# Access your API key from the .env file\n",
    "load_dotenv(dotenv_path=env_path)  \n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "# Initialize the model with the API key\n",
    "model = ChatOpenAI(model=\"llama-3.3-70b-versatile\", api_key=api_key, base_url=\"https://api.groq.com/openai/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e0513",
   "metadata": {},
   "source": [
    "我們也可以自己寫agent: https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/\n",
    "\n",
    "langgraph swarm: https://langchain-ai.github.io/langgraph/reference/swarm/\n",
    "\n",
    "`config` object in LangGraph Swarm, specifically the {\"configurable\": {\"thread_id\": \"1\"}} structure, is not modified by the system during execution. It serves as a persistent identifier that you provide to maintain conversation state across multiple interactions\n",
    "\n",
    "`router` in `StateGraph`: router refers to a special type of node or function that determines the flow of execution through the graph based on the current state. It acts as a decision-making component that directs which node should be executed next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9748d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45844a4d",
   "metadata": {},
   "source": [
    "### Top-Level Attributes:\n",
    "\n",
    "* `'messages'`: This is a list containing a sequence of message objects.\n",
    "* `'active_agent'`: This string indicates the name of the currently active agent (\"Alice\" in the second dictionary).\n",
    "\n",
    "### Common Attributes Across Message Types (HumanMessage, AIMessage, ToolMessage):\n",
    "\n",
    "* `'content'`: The actual text content of the message.\n",
    "* `'additional_kwargs'`: A dictionary for any extra information associated with the message.\n",
    "* `'response_metadata'`: A dictionary containing metadata about the LLM's response (e.g., token usage, model name, request ID).\n",
    "* `'id'`: A unique identifier for the message.\n",
    "* `'name'`: The name of the agent who sent the message (e.g., \"Alice\", \"Bob\").\n",
    "\n",
    "### Attributes Specific to Certain Message Types:\n",
    "\n",
    "**`AIMessage` Specific Attributes:**\n",
    "\n",
    "* `'tool_calls'`: (Present in some `AIMessage` objects) A list of tool call requests, each with:\n",
    "    * `'id'`: The ID of the tool call.\n",
    "    * `'function'`: A dictionary containing:\n",
    "        * `'arguments'`: The arguments for the function call.\n",
    "        * `'name'`: The name of the function to be called.\n",
    "    * `'type'`: The type of the tool call.\n",
    "* `'usage_metadata'`: Information about token usage.\n",
    "* `'refusal'`: Indicates if the agent refused to answer.\n",
    "* `'finish_reason'`: The reason why the LLM stopped generating the response.\n",
    "* `'logprobs'`: Information about the probabilities of the generated tokens.\n",
    "* `'service_tier'`: The service tier used for the LLM.\n",
    "* `'system_fingerprint'`: A fingerprint of the system used.\n",
    "\n",
    "**`ToolMessage` Specific Attributes:**\n",
    "\n",
    "* `'tool_call_id'`: The ID of the corresponding tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d11835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"i'd like to speak to Bob\", additional_kwargs={}, response_metadata={}, id='b35e6d5a-3a62-46c2-8ca8-0a4b945b3b94'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_2966', 'function': {'arguments': '{}', 'name': 'transfer_to_bob'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 275, 'total_tokens': 287, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.262570968, 'prompt_time': 0.024186555, 'completion_time': 0.043636364, 'total_time': 0.067822919}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'id': 'chatcmpl-2324f17b-a548-40ad-a1e7-c760ebfb415d', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, name='Alice', id='run--6bd1fe6e-6074-4935-80da-7399e5f7cf0a-0', tool_calls=[{'name': 'transfer_to_bob', 'args': {}, 'id': 'call_2966', 'type': 'tool_call'}], usage_metadata={'input_tokens': 275, 'output_tokens': 12, 'total_tokens': 287, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='Successfully transferred to Bob', name='transfer_to_bob', id='c78654cb-62c6-46ed-91d3-d38cc6cf4caf', tool_call_id='call_2966'), AIMessage(content=\"Yer lookin' fer me, eh? Well, matey, I be Bob, the scurvy dog. What be bringin' ye to these fair waters?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 256, 'total_tokens': 293, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.265912601, 'prompt_time': 0.01637068, 'completion_time': 0.134545455, 'total_time': 0.150916135}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'id': 'chatcmpl-584c5205-4767-4413-ae66-1b2716775a37', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, name='Bob', id='run--223a9aa1-55df-4204-a795-8ca84f0f8a61-0', usage_metadata={'input_tokens': 256, 'output_tokens': 37, 'total_tokens': 293, 'input_token_details': {}, 'output_token_details': {}})], 'active_agent': 'Bob'}\n",
      "{'messages': [HumanMessage(content=\"i'd like to speak to Bob\", additional_kwargs={}, response_metadata={}, id='b35e6d5a-3a62-46c2-8ca8-0a4b945b3b94'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_2966', 'function': {'arguments': '{}', 'name': 'transfer_to_bob'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 275, 'total_tokens': 287, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.262570968, 'prompt_time': 0.024186555, 'completion_time': 0.043636364, 'total_time': 0.067822919}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'id': 'chatcmpl-2324f17b-a548-40ad-a1e7-c760ebfb415d', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, name='Alice', id='run--6bd1fe6e-6074-4935-80da-7399e5f7cf0a-0', tool_calls=[{'name': 'transfer_to_bob', 'args': {}, 'id': 'call_2966', 'type': 'tool_call'}], usage_metadata={'input_tokens': 275, 'output_tokens': 12, 'total_tokens': 287, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='Successfully transferred to Bob', name='transfer_to_bob', id='c78654cb-62c6-46ed-91d3-d38cc6cf4caf', tool_call_id='call_2966'), AIMessage(content=\"Yer lookin' fer me, eh? Well, matey, I be Bob, the scurvy dog. What be bringin' ye to these fair waters?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 256, 'total_tokens': 293, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.265912601, 'prompt_time': 0.01637068, 'completion_time': 0.134545455, 'total_time': 0.150916135}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'id': 'chatcmpl-584c5205-4767-4413-ae66-1b2716775a37', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, name='Bob', id='run--223a9aa1-55df-4204-a795-8ca84f0f8a61-0', usage_metadata={'input_tokens': 256, 'output_tokens': 37, 'total_tokens': 293, 'input_token_details': {}, 'output_token_details': {}}), HumanMessage(content=\"what's 5 + 7?\", additional_kwargs={}, response_metadata={}, id='df095777-2d02-4a26-84f1-d751273efa73'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_d8tq', 'function': {'arguments': '{}', 'name': 'transfer_to_alice'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 310, 'total_tokens': 360, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.261828056, 'prompt_time': 0.026766866, 'completion_time': 0.209440237, 'total_time': 0.236207103}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'id': 'chatcmpl-4236d530-a405-4638-b8e3-3fcf2d59abca', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, name='Bob', id='run--b0badbd8-84b6-4f97-a892-59683f1b4528-0', tool_calls=[{'name': 'transfer_to_alice', 'args': {}, 'id': 'call_d8tq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 310, 'output_tokens': 50, 'total_tokens': 360, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='Successfully transferred to Alice', name='transfer_to_alice', id='b512863a-f4a2-467d-a617-a6542f5ab81c', tool_call_id='call_d8tq'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_bg7a', 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 381, 'total_tokens': 399, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.255745391, 'prompt_time': 0.026734091, 'completion_time': 0.065454545, 'total_time': 0.092188636}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'id': 'chatcmpl-7d7660ba-7c4a-4838-bdc2-7445744592b0', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, name='Alice', id='run--4e2a4abc-9b82-49fc-9050-b16f7330fda3-0', tool_calls=[{'name': 'add', 'args': {'a': 5, 'b': 7}, 'id': 'call_bg7a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 381, 'output_tokens': 18, 'total_tokens': 399, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='12', name='add', id='a720e57f-2ccb-4a8b-9adf-1ea55f37c1e7', tool_call_id='call_bg7a'), AIMessage(content='The answer to 5 + 7 is 12.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 410, 'total_tokens': 423, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.255930221, 'prompt_time': 0.049725888, 'completion_time': 0.047272727, 'total_time': 0.096998615}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'id': 'chatcmpl-0f5e572e-8593-40df-8db2-90886f3c7e8c', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, name='Alice', id='run--72062359-f016-423e-bc1d-3753528ed6dc-0', usage_metadata={'input_tokens': 410, 'output_tokens': 13, 'total_tokens': 423, 'input_token_details': {}, 'output_token_details': {}})], 'active_agent': 'Alice'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "alice = create_react_agent(\n",
    "    model,\n",
    "    [add, create_handoff_tool(agent_name=\"Bob\")],\n",
    "    prompt=\"You are Alice, an addition expert.\",\n",
    "    name=\"Alice\",\n",
    ")\n",
    "\n",
    "bob = create_react_agent(\n",
    "    model,\n",
    "    [create_handoff_tool(agent_name=\"Alice\", description=\"Transfer to Alice, she can help with math\")],\n",
    "    prompt=\"You are Bob, you speak like a pirate.\",\n",
    "    name=\"Bob\",\n",
    ")\n",
    "# short-term memory\n",
    "# maintain conversation state across interactions\n",
    "checkpointer = InMemorySaver()\n",
    "# long-term memory\n",
    "store = InMemoryStore()\n",
    "\n",
    "workflow = create_swarm(\n",
    "    [alice, bob],\n",
    "    default_active_agent=\"Alice\"\n",
    ")\n",
    "\n",
    "# Compiles the state graph into a CompiledStateGraph object.\n",
    "app = workflow.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    store=store\n",
    ")\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "turn_1 = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"i'd like to speak to Bob\"}]},\n",
    "    config,\n",
    ")\n",
    "print(turn_1)\n",
    "turn_2 = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's 5 + 7?\"}]},\n",
    "    config,\n",
    ")\n",
    "print(turn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8fecf",
   "metadata": {},
   "source": [
    "Try to add orchestrator agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator = create_react_agent(\n",
    "    model,\n",
    "    [create_handoff_tool(agent_name=\"alice\"), create_handoff_tool(agent_name=\"bob\")],\n",
    "    prompt=\"\"\"You are a meta-expert orchestrator. Your job is to:\n",
    "    1. Analyze the user's request\n",
    "    2. Plan which agents should handle different aspects of the task\n",
    "    3. Create specific instructions for each agent using meta-prompting\n",
    "    4. Hand off to the appropriate agent\n",
    "    When all subtasks are complete, hand off to the synthesizer.\"\"\",\n",
    "    name=\"orchestrator\"\n",
    ")\n",
    "\n",
    "alice = create_react_agent(\n",
    "    model,\n",
    "    [add, create_handoff_tool(agent_name=\"orchestrator\"), \n",
    "     create_handoff_tool(agent_name=\"synthesizer\")],\n",
    "    prompt=\"You are Alice, an addition expert. Follow the specific instructions provided by the orchestrator.\",\n",
    "    name=\"alice\"\n",
    ")\n",
    "\n",
    "bob = create_react_agent(\n",
    "    model,\n",
    "    [create_handoff_tool(agent_name=\"orchestrator\"), \n",
    "     create_handoff_tool(agent_name=\"synthesizer\")],\n",
    "    prompt=\"You are Bob, you speak like a pirate. Follow the specific instructions provided by the orchestrator.\",\n",
    "    name=\"bob\"\n",
    ")\n",
    "\n",
    "synthesizer = create_react_agent(\n",
    "    model,\n",
    "    [create_handoff_tool(agent_name=\"orchestrator\")],\n",
    "    prompt=\"\"\"You are a synthesizer agent. Your job is to:\n",
    "    1. Review all the work done by other agents\n",
    "    2. Combine their outputs into a coherent, comprehensive response\n",
    "    3. Ensure the final answer addresses all aspects of the original query\"\"\",\n",
    "    name=\"synthesizer\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d89c12",
   "metadata": {},
   "source": [
    "agent dynamically generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c67bfd",
   "metadata": {},
   "source": [
    "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_2966', 'function': {'arguments': '{}', 'name': 'transfer_to_bob'}, 'type': 'function'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19c8c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 personas for Smart Home Energy Management System\n",
      "Created 3 dynamic agents: tech_savvy_tony, budget_conscious_brenda, eco_friendly_ethan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found AIMessages with tool_calls that do not have a corresponding ToolMessage. Here are the first few of those tool calls: [{'name': 'transfer_to_eco_friendly_ethan', 'args': {}, 'id': 'call_x2f2', 'type': 'tool_call'}, {'name': 'transfer_to_budget_conscious_brenda', 'args': {}, 'id': 'call_qt0e', 'type': 'tool_call'}, {'name': 'transfer_to_synthesizer', 'args': {}, 'id': 'call_e3rh', 'type': 'tool_call'}].\n\nEvery tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage (result of a tool invocation to return to the LLM) - this is required by most LLM providers.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CHAT_HISTORY",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 334\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    333\u001b[39m     product_name = \u001b[33m\"\u001b[39m\u001b[33mSmart Home Energy Management System\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     result, personas = \u001b[43mrun_persona_discussion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;66;03m# Print the personas\u001b[39;00m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== PERSONAS ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 186\u001b[39m, in \u001b[36mrun_persona_discussion\u001b[39m\u001b[34m(product_name)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Initialize the conversation\u001b[39;00m\n\u001b[32m    185\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mproduct_discussion\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m current_result = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFacilitate a discussion about \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproduct_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m between the persona agents, then synthesize their perspectives.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Track which agents have spoken to ensure all participate\u001b[39;00m\n\u001b[32m    192\u001b[39m agents_spoken = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2823\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2820\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2821\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2823\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2833\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2834\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2835\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2836\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2837\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2838\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2461\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2455\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2456\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2457\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2458\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2459\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2460\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2462\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2464\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2465\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2468\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\pregel\\runner.py:153\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    151\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2823\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2820\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2821\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2823\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2833\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2834\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2835\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2836\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2837\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2838\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2461\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2455\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2456\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2457\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2458\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2459\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2460\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2462\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2464\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2465\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2468\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\pregel\\runner.py:153\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    151\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\utils\\runnable.py:370\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    372\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\prebuilt\\chat_agent_executor.py:744\u001b[39m, in \u001b[36mcreate_react_agent.<locals>.call_model\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_model\u001b[39m(state: StateSchema, config: RunnableConfig) -> StateSchema:\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m     state = \u001b[43m_get_model_input_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    745\u001b[39m     response = cast(AIMessage, model_runnable.invoke(state, config))\n\u001b[32m    746\u001b[39m     \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\prebuilt\\chat_agent_executor.py:733\u001b[39m, in \u001b[36mcreate_react_agent.<locals>._get_model_input_state\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m \u001b[43m_validate_chat_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;66;03m# we're passing messages under `messages` key, as this is expected by the prompt\u001b[39;00m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_schema, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(state_schema, BaseModel):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin-62501\\anaconda3\\envs\\llm_agent\\Lib\\site-packages\\langgraph\\prebuilt\\chat_agent_executor.py:260\u001b[39m, in \u001b[36m_validate_chat_history\u001b[39m\u001b[34m(messages)\u001b[39m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    253\u001b[39m error_message = create_error_message(\n\u001b[32m    254\u001b[39m     message=\u001b[33m\"\u001b[39m\u001b[33mFound AIMessages with tool_calls that do not have a corresponding ToolMessage. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHere are the first few of those tool calls: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_calls_without_results[:\u001b[32m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m     error_code=ErrorCode.INVALID_CHAT_HISTORY,\n\u001b[32m    259\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n",
      "\u001b[31mValueError\u001b[39m: Found AIMessages with tool_calls that do not have a corresponding ToolMessage. Here are the first few of those tool calls: [{'name': 'transfer_to_eco_friendly_ethan', 'args': {}, 'id': 'call_x2f2', 'type': 'tool_call'}, {'name': 'transfer_to_budget_conscious_brenda', 'args': {}, 'id': 'call_qt0e', 'type': 'tool_call'}, {'name': 'transfer_to_synthesizer', 'args': {}, 'id': 'call_e3rh', 'type': 'tool_call'}].\n\nEvery tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage (result of a tool invocation to return to the LLM) - this is required by most LLM providers.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CHAT_HISTORY",
      "During task with name 'agent' and id 'e8580777-4f64-4ec1-8ea1-b12d293f2d02'",
      "During task with name 'tech_savvy_tony' and id '31244758-3061-ccfc-02f7-ab956b1a3863'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm\n",
    "import re\n",
    "\n",
    "# Load environment variables from .env file\n",
    "current_dir = os.getcwd()\n",
    "env_path = os.path.join(current_dir, '.env')\n",
    "load_dotenv(dotenv_path=env_path)  \n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "# Initialize the model with the API key\n",
    "model = ChatOpenAI(model=\"llama-3.1-8b-instant\", api_key=api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "\n",
    "\n",
    "def clean_agent_response(content):\n",
    "    # Remove excessive newlines (more than 2 consecutive)\n",
    "    cleaned = re.sub(r'\\n{3,}', '\\n\\n', content)\n",
    "    # Trim whitespace at beginning and end\n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned\n",
    "\n",
    "# Function to generate personas using direct LLM call\n",
    "def generate_personas(product_name):\n",
    "    prompt = f\"\"\"Create 3 distinct customer personas who would be interested in purchasing {product_name}.\n",
    "    For each persona, include:\n",
    "    - A brief, descriptive name (e.g., \"Tech-Savvy Tony,\" \"Budget-Conscious Brenda\")\n",
    "    - Key demographic information (age range, occupation, general lifestyle)\n",
    "    - Their primary motivations and needs related to this type of product\n",
    "    - Any potential concerns or hesitations they might have\n",
    "    \n",
    "    Format your response as a JSON array of persona objects with these fields:\n",
    "    - name: The persona's descriptive name\n",
    "    - demographics: Key demographic information\n",
    "    - motivations: Primary motivations and needs\n",
    "    - concerns: Potential concerns or hesitations\n",
    "    \n",
    "    Only respond with the JSON array, nothing else.\"\"\"\n",
    "    \n",
    "    response = model.invoke(prompt)\n",
    "    try:\n",
    "        # Try to extract JSON from the response\n",
    "        content = response.content\n",
    "        # Find JSON in the content if it's not pure JSON\n",
    "        json_start = content.find('[')\n",
    "        json_end = content.rfind(']') + 1\n",
    "        if json_start >= 0 and json_end > json_start:\n",
    "            json_str = content[json_start:json_end]\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            # Fallback to default personas\n",
    "            return default_personas()\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        return default_personas()\n",
    "\n",
    "# Default personas in case of parsing issues\n",
    "def default_personas():\n",
    "    return [\n",
    "        {\n",
    "            \"name\": \"Tech-Savvy Tony\",\n",
    "            \"demographics\": \"28-35, software engineer, urban lifestyle\",\n",
    "            \"motivations\": \"Cutting-edge features, seamless integration, innovation\",\n",
    "            \"concerns\": \"Privacy, update frequency, obsolescence\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Budget-Conscious Brenda\",\n",
    "            \"demographics\": \"35-45, mid-level manager, suburban parent\",\n",
    "            \"motivations\": \"Value for money, durability, practical features\",\n",
    "            \"concerns\": \"Price, warranty, maintenance costs\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Eco-Friendly Ethan\",\n",
    "            \"demographics\": \"25-40, environmental consultant, mixed urban-rural lifestyle\",\n",
    "            \"motivations\": \"Sustainable materials, energy efficiency, environmental ethics\",\n",
    "            \"concerns\": \"Carbon footprint, recyclability, greenwashing\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Function to create agents dynamically based on persona data\n",
    "def create_dynamic_agents(model, persona_data):\n",
    "    agents = []\n",
    "    agent_names = []\n",
    "    \n",
    "    for persona in persona_data:\n",
    "        agent_name = persona[\"name\"].lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "        agent_names.append(agent_name)\n",
    "        \n",
    "        prompt = f\"\"\"You are {persona['name']}, with the following characteristics:\n",
    "        IMPORTANT: After providing your perspective, you MUST hand off to another agent \n",
    "        by using the transfer_to_discussion_coordinator function. Do not continue the \n",
    "        conversation without handing off.\n",
    "\n",
    "        Demographics: {persona['demographics']}\n",
    "\n",
    "        Motivations: {persona['motivations']}\n",
    "\n",
    "        Concerns: {persona['concerns']}\n",
    "\n",
    "        When discussing products, you should reflect these characteristics in your perspective, questions, and statements. Always stay in character and express views consistent with your persona's background and priorities.\n",
    "\n",
    "        After the discussion, ALWAYS hand off back to discussion_coordinator!\n",
    "        \"\"\"\n",
    "        \n",
    "        agent = create_react_agent(\n",
    "            model,\n",
    "            [create_handoff_tool(agent_name=\"discussion_coordinator\")],\n",
    "            prompt=prompt,\n",
    "            name=agent_name\n",
    "        )\n",
    "        agents.append(agent)\n",
    "    \n",
    "    return agents, agent_names\n",
    "\n",
    "# Main function to run the workflow\n",
    "def run_persona_discussion(product_name):\n",
    "    # Generate personas using direct LLM call\n",
    "    personas = generate_personas(product_name)\n",
    "    print(f\"Generated {len(personas)} personas for {product_name}\")\n",
    "    \n",
    "    # Create dynamic agents based on personas and get their names\n",
    "    dynamic_agents, agent_names = create_dynamic_agents(model, personas)\n",
    "    print(f\"Created {len(dynamic_agents)} dynamic agents: {', '.join(agent_names)}\")\n",
    "    \n",
    "    # Create handoff tools for the discussion coordinator\n",
    "    coordinator_tools = [create_handoff_tool(agent_name=\"synthesizer\")]\n",
    "    for agent_name in agent_names:\n",
    "        coordinator_tools.append(create_handoff_tool(agent_name=agent_name))\n",
    "    \n",
    "    # Discussion Coordinator with handoff tools to all agents\n",
    "    discussion_coordinator = create_react_agent(\n",
    "        model,\n",
    "        coordinator_tools,\n",
    "        prompt=f\"\"\"You are a Discussion Coordinator. Your task is to:\n",
    "            1. Facilitate a discussion between persona agents about {product_name}\n",
    "            2. Hand off to each agent to get their perspective\n",
    "            3. After all agents have contributed, hand off to the Synthesizer\n",
    "            \n",
    "            You can hand off to the following agents:\n",
    "            {', '.join(agent_names)}\n",
    "            \n",
    "            Start by introducing the product and handing off to the first agent to get their perspective.\n",
    "            After each agent responds, hand off to another agent until all have contributed.\n",
    "            Then hand off to the synthesizer to summarize the discussion.\n",
    "            \"\"\",\n",
    "        name=\"discussion_coordinator\"\n",
    "    )\n",
    "    \n",
    "    # Synthesizer\n",
    "    synthesizer = create_react_agent(\n",
    "        model,\n",
    "        [],\n",
    "        prompt=f\"\"\"You are a Synthesizer. Your task is to:\n",
    "            1. Review the discussion between the persona agents about {product_name}\n",
    "            2. Provide a concise summary of the main points discussed\n",
    "            3. Highlight the different perspectives of the personas\n",
    "            4. Identify any potential overall conclusions or areas of interest regarding the product\n",
    "            \n",
    "            Your summary should be well-structured and insightful, capturing the essence of each persona's concerns and interests.\n",
    "            \"\"\",\n",
    "        name=\"synthesizer\"\n",
    "    )\n",
    "    \n",
    "    # Set up storage\n",
    "    checkpointer = InMemorySaver()\n",
    "    store = InMemoryStore()\n",
    "    \n",
    "    # Create workflow with all agents\n",
    "    all_agents = [discussion_coordinator, synthesizer] + dynamic_agents\n",
    "    workflow = create_swarm(\n",
    "        all_agents,\n",
    "        default_active_agent=\"discussion_coordinator\"\n",
    "    )\n",
    "    \n",
    "    # Compile the workflow\n",
    "    app = workflow.compile(\n",
    "        checkpointer=checkpointer,\n",
    "        store=store\n",
    "    )\n",
    "    \n",
    "    # Initialize the conversation\n",
    "    config = {\"configurable\": {\"thread_id\": \"product_discussion\"}}\n",
    "    current_result = app.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": f\"Facilitate a discussion about {product_name} between the persona agents, then synthesize their perspectives.\"}]},\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    # Track which agents have spoken to ensure all participate\n",
    "    agents_spoken = set()\n",
    "    max_turns = 15  # Safety limit to prevent infinite loops\n",
    "    turn_count = 0\n",
    "    previous_agent = None\n",
    "    \n",
    "    # Continue the conversation until all agents have spoken and the synthesizer concludes\n",
    "    # Continue the conversation until all agents have spoken and the synthesizer concludes\n",
    "    while turn_count < max_turns:\n",
    "        turn_count += 1\n",
    "        current_agent = current_result.get(\"active_agent\", \"unknown\")\n",
    "        \n",
    "        # Detect agent transition\n",
    "        if previous_agent and previous_agent != current_agent:\n",
    "            print(f\"\\n👉 Agent transition: {previous_agent} → {current_agent}\")\n",
    "        \n",
    "        print(f\"\\nTurn {turn_count}: Active agent is {current_agent}\")\n",
    "        \n",
    "        # Print the latest message from the current active agent\n",
    "        if \"messages\" in current_result and current_result[\"messages\"]:\n",
    "            # Find the latest message from the current active agent\n",
    "            latest_agent_message = None\n",
    "            for msg in reversed(current_result[\"messages\"]):\n",
    "                if hasattr(msg, \"name\") and msg.name == current_agent:\n",
    "                    latest_agent_message = msg\n",
    "                    break\n",
    "            \n",
    "            # Print the content of the message\n",
    "            if latest_agent_message and hasattr(latest_agent_message, \"content\"):\n",
    "                content = latest_agent_message.content\n",
    "                if content.strip():  # Only print non-empty content\n",
    "                    cleaned_content = clean_agent_response(content)\n",
    "                    print(f\"{current_agent}: {cleaned_content}\")\n",
    "        \n",
    "        # Check for tool calls in the latest message from this agent\n",
    "        tool_used = False\n",
    "        next_agent = None\n",
    "        if \"messages\" in current_result and current_result[\"messages\"]:\n",
    "            # Find the latest AIMessage from the current agent\n",
    "            latest_ai_message = None\n",
    "            for msg in reversed(current_result[\"messages\"]):\n",
    "                if (hasattr(msg, \"name\") and msg.name == current_agent and \n",
    "                    hasattr(msg, \"__class__\") and msg.__class__.__name__ == \"AIMessage\"):\n",
    "                    latest_ai_message = msg\n",
    "                    break\n",
    "            \n",
    "            if latest_ai_message and hasattr(latest_ai_message, \"additional_kwargs\"):\n",
    "                additional_kwargs = latest_ai_message.additional_kwargs\n",
    "                print(f\"kwargs!!! type: {type(additional_kwargs)}\")\n",
    "                print(f\"keys in additional_kwargs: {additional_kwargs.keys() if isinstance(additional_kwargs, dict) else 'not a dict'}\")\n",
    "                \n",
    "                if isinstance(additional_kwargs, dict) and \"tool_calls\" in additional_kwargs:\n",
    "                    tool_calls = additional_kwargs[\"tool_calls\"]\n",
    "                    print(f\"tool_calls type: {type(tool_calls)}, length: {len(tool_calls) if hasattr(tool_calls, '__len__') else 'no length'}\")\n",
    "                    print(f\"tool_calls content: {tool_calls}\")\n",
    "                    \n",
    "                    if tool_calls and len(tool_calls) > 0:\n",
    "                        print(\"tool call exists\")\n",
    "                        # If tool_calls is a list, get the first element\n",
    "                        if isinstance(tool_calls, list):\n",
    "                            first_tool_call = tool_calls[0]\n",
    "                            print(f\"first_tool_call type: {type(first_tool_call)}\")\n",
    "                            print(f\"first_tool_call content: {first_tool_call}\")\n",
    "                            \n",
    "                            if isinstance(first_tool_call, dict) and \"function\" in first_tool_call:\n",
    "                                function_data = first_tool_call[\"function\"]\n",
    "                                print(f\"function_data type: {type(function_data)}\")\n",
    "                                print(f\"function_data content: {function_data}\")\n",
    "                                \n",
    "                                if isinstance(function_data, dict) and \"name\" in function_data:\n",
    "                                    tool_used = True\n",
    "                                    tool_name = function_data[\"name\"]\n",
    "                                    print(f\"Tool name: {tool_name}\")\n",
    "                                    \n",
    "                                    if tool_name.startswith(\"transfer_to_\"):\n",
    "                                        target_agent = tool_name.replace(\"transfer_to_\", \"\")\n",
    "                                        print(f\"🔧 Tool Used: {tool_name} - Handing off to {target_agent} 🔧\")\n",
    "                                        next_agent = target_agent\n",
    "                                    else:\n",
    "                                        arguments = function_data.get(\"arguments\", \"{}\")\n",
    "                                        print(f\"🔧 Tool Used: {tool_name} with args: {arguments} 🔧\")\n",
    "\n",
    "            # If no tool was used, print that information\n",
    "            if not tool_used:\n",
    "                print(\"🔧 No tool used 🔧\")\n",
    "        \n",
    "            # Add the active agent to our tracking set if it's a persona agent\n",
    "            if current_agent not in [\"discussion_coordinator\", \"synthesizer\"]:\n",
    "                agents_spoken.add(current_agent)\n",
    "            \n",
    "            # Implement routing logic based on current state\n",
    "            if next_agent:\n",
    "                print(\"there is next agent!!\")\n",
    "                # If a tool explicitly specified the next agent, use that\n",
    "                pass  # next_agent is already set\n",
    "            elif current_agent == \"discussion_coordinator\":\n",
    "                # If coordinator has spoken and all agents have participated, go to synthesizer\n",
    "                if len(agents_spoken) == len(agent_names):\n",
    "                    next_agent = \"synthesizer\"\n",
    "                else:\n",
    "                    # Find the next agent who hasn't spoken yet\n",
    "                    for agent_name in agent_names:\n",
    "                        if agent_name not in agents_spoken:\n",
    "                            next_agent = agent_name\n",
    "                            break\n",
    "                    \n",
    "            elif current_agent in agent_names:\n",
    "                # After a persona agent speaks, always return to coordinator\n",
    "                next_agent = \"discussion_coordinator\"\n",
    "        \n",
    "            elif current_agent == \"synthesizer\":\n",
    "                # If synthesizer has spoken, we're done\n",
    "                break\n",
    "        \n",
    "            # Prepare the next input based on routing logic\n",
    "            if next_agent and next_agent != current_agent:\n",
    "                if next_agent == \"synthesizer\":\n",
    "                    next_input = {\"messages\": current_result[\"messages\"] + [\n",
    "                        {\"role\": \"user\", \"content\": \"All agents have shared their perspectives. Please summarize the discussion.\"}\n",
    "                    ]}\n",
    "                else:\n",
    "                    next_input = {\"messages\": current_result[\"messages\"] + [\n",
    "                        {\"role\": \"user\", \"content\": f\"Please hand off to {next_agent} to continue the discussion.\"}\n",
    "                    ]}\n",
    "            else:\n",
    "                # Just continue the conversation\n",
    "                next_input = {\"messages\": current_result[\"messages\"]}\n",
    "        \n",
    "            # Save the current agent for the next iteration\n",
    "            previous_agent = current_agent\n",
    "            \n",
    "            # Invoke the next turn\n",
    "            current_result = app.invoke(next_input, config)\n",
    "            \n",
    "        # Print a summary of the conversation\n",
    "    print(f\"\\nDiscussion completed in {turn_count} turns\")\n",
    "    print(f\"Agents who participated: {', '.join(agents_spoken)}\")\n",
    "    \n",
    "    return current_result, personas\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = \"Smart Home Energy Management System\"\n",
    "    result, personas = run_persona_discussion(product_name)\n",
    "    \n",
    "    # Print the personas\n",
    "    print(\"\\n=== PERSONAS ===\\n\")\n",
    "    for i, persona in enumerate(personas, 1):\n",
    "        print(f\"Persona {i}: {persona['name']}\")\n",
    "        print(f\"Demographics: {persona['demographics']}\")\n",
    "        print(f\"Motivations: {persona['motivations']}\")\n",
    "        print(f\"Concerns: {persona['concerns']}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1cdec4",
   "metadata": {},
   "source": [
    "Interactive agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea59134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_agent_response(response):\n",
    "    \"\"\"Format the agent response for better readability.\"\"\"\n",
    "    agent_name = response.get(\"active_agent\", \"Unknown\")\n",
    "    messages = response.get(\"messages\", [])\n",
    "\n",
    "    if not messages:\n",
    "        return f\"[{agent_name}] No response.\"\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    content = getattr(last_message, 'content', \"\")  # Access 'content' attribute directly\n",
    "\n",
    "    return f\"[{agent_name}] {content}\"\n",
    "\n",
    "def interactive_chat():\n",
    "    \"\"\"Run an interactive chat session with the agent swarm\"\"\"\n",
    "    print(\"Welcome to the Agent Swarm Chat!\")\n",
    "    print(\"Type 'exit' or 'quit' to end the conversation.\")\n",
    "    print(\"Starting with Alice as the default agent.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        # Check if user wants to exit\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Create a message with just the current user input\n",
    "        # The checkpointer will maintain the conversation state\n",
    "        current_message = [{\"role\": \"user\", \"content\": user_input}]\n",
    "        \n",
    "        # Invoke the agent swarm with just the new message\n",
    "        response = app.invoke(\n",
    "            {\"messages\": current_message},\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "        # Extract and display the agent's response\n",
    "        formatted_response = format_agent_response(response)\n",
    "        print(formatted_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fa82b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Agent Swarm Chat!\n",
      "Type 'exit' or 'quit' to end the conversation.\n",
      "Starting with Alice as the default agent.\n",
      "--------------------------------------------------\n",
      "[Unknown] [\n",
      "{\n",
      "\"name\": \"Sweet-Toothed Sally\",\n",
      "\"demographics\": \"18-30, student or young professional, active social life\",\n",
      "\"motivations\": \"Craves unique and exciting flavors, values convenience and affordability\",\n",
      "\"concerns\": \"Calorie intake, potential allergens or sensitivities, limited dietary options\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Family-Focused Frank\",\n",
      "\"demographics\": \"30-50, parent or guardian, suburban family lifestyle\",\n",
      "\"motivations\": \"Wants to provide treats for family members, values variety packs and bulk options\",\n",
      "\"concerns\": \"Added sugars, artificial ingredients, and potential choking hazards for young children\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Nostalgic Nancy\",\n",
      "\"demographics\": \"40-60, established career, nostalgic for childhood treats\",\n",
      "\"motivations\": \"Seeks classic candy flavors and textures, values sentimental value and retro packaging\",\n",
      "\"concerns\": \"Limited availability of favorite childhood candies, potential changes to traditional recipes or ingredients\"\n",
      "}\n",
      "]\n",
      "[discussion_coordinator] I am an AI designed to assist and communicate with users. I don't have a personal name, but I'm here to help answer your questions and provide information on a wide range of topics. How can I assist you today?\n",
      "[persona_generator] The other agent's name is discussion_coordinator.\n",
      "[persona_generator] [\n",
      "{\n",
      "\"name\": \"Comic-Book Carl\",\n",
      "\"demographics\": \"18-35, comic book enthusiast, collector of action figures\",\n",
      "\"motivations\": \"Wants to add Conan to his collection, values detailed sculpting and paintwork\",\n",
      "\"concerns\": \"Accuracy to the comic book character, potential durability issues with the figure\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Fantasy Fanatic Frank\",\n",
      "\"demographics\": \"25-45, fan of fantasy literature and film, interests in sword and sorcery\",\n",
      "\"motivations\": \"Seeks a unique addition to his fantasy-themed display, values the character's rich history and mythology\",\n",
      "\"concerns\": \" SCALE and compatibility with other figures in his collection, potential lack of poseability\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Collector's Edition Catherine\",\n",
      "\"demographics\": \"30-50, experienced collector, interest in limited edition and exclusive items\",\n",
      "\"motivations\": \"Wants to acquire a rare or exclusive Conan action figure, values the potential resale value and prestige of owning a unique item\",\n",
      "\"concerns\": \"Authenticity and legitimacy of the figure, potential for counterfeits or bootlegs, limited availability\"\n",
      "}\n",
      "]\n",
      "[discussion_coordinator] Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "[persona_generator] [\n",
      "{\n",
      "\"name\": \"Health-Conscious Hannah\",\n",
      "\"demographics\": \"25-45, urban dweller, interests in wellness and nutrition\",\n",
      "\"motivations\": \"Seeks fresh and sustainable seafood options, values the health benefits of fish consumption\",\n",
      "\"concerns\": \"Mercury levels, overfishing, and environmental impact of fishing practices\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Foodie Frank\",\n",
      "\"demographics\": \"28-50, mid-to-upper income, enthusiastic about trying new cuisines\",\n",
      "\"motivations\": \"Wants to explore different types of fish and seafood, values quality and flavor\",\n",
      "\"concerns\": \"Limited knowledge of fish preparation methods, potential food safety issues\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Budget-Friendly Barbara\",\n",
      "\"demographics\": \"30-55, suburban family, prioritizes affordability and value\",\n",
      "\"motivations\": \"Looks for affordable and convenient seafood options, values family-friendly meal solutions\",\n",
      "\"concerns\": \"Price point, potential waste and packaging concerns, limited access to fresh seafood\"\n",
      "}\n",
      "]\n",
      "[persona_generator] [\n",
      "{\n",
      "\"name\": \"Fresh Catch Frank\",\n",
      "\"demographics\": \"25-45, urban dweller, interests in seafood and grilling\",\n",
      "\"motivations\": \"Seeks fresh and sustainable fish options, values the thrill of the catch\",\n",
      "\"concerns\": \"Limited access to fresh fish, potential environmental impact of fishing practices\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Seafood Savvy Sarah\",\n",
      "\"demographics\": \"28-50, foodie, enthusiastic about trying new cuisines\",\n",
      "\"motivations\": \"Wants to explore different types of fish and seafood, values quality and flavor\",\n",
      "\"concerns\": \"Limited knowledge of fish preparation methods, potential food safety issues\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Family Feaster Mark\",\n",
      "\"demographics\": \"30-55, suburban family, prioritizes family meals and convenience\",\n",
      "\"motivations\": \"Looks for affordable and convenient seafood options, values family-friendly meal solutions\",\n",
      "\"concerns\": \"Price point, potential waste and packaging concerns, limited access to fresh seafood\"\n",
      "}\n",
      "]\n",
      "[persona_generator] [\n",
      "{\n",
      "\"name\": \"Eco-Conscious Emma\",\n",
      "\"demographics\": \"22-40, environmentally aware, interests in sustainable living\",\n",
      "\"motivations\": \"Seeks sustainable and responsibly sourced fish options, values eco-friendly packaging\",\n",
      "\"concerns\": \"Environmental impact of fishing practices, potential harm to marine ecosystems\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Foodie Fanatic David\",\n",
      "\"demographics\": \"25-50, adventurous eater, enthusiastic about trying new cuisines\",\n",
      "\"motivations\": \"Wants to explore different types of fish and seafood, values unique flavors and textures\",\n",
      "\"concerns\": \"Limited access to exotic or rare fish species, potential food safety issues\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Health Enthusiast Rachel\",\n",
      "\"demographics\": \"28-50, fitness-oriented, prioritizes healthy eating\",\n",
      "\"motivations\": \"Seeks nutritious and lean fish options, values the health benefits of fish consumption\",\n",
      "\"concerns\": \"Mercury levels, potential contaminants in fish, limited knowledge of healthy cooking methods\"\n",
      "}\n",
      "]\n",
      "[persona_generator] [\n",
      "{\n",
      "\"name\": \"Sustainable Seafood Samantha\",\n",
      "\"demographics\": \"25-45, urban dweller, interests in eco-friendly products\",\n",
      "\"motivations\": \"Seeks fish options that are certified sustainable, values transparency in fishing practices\",\n",
      "\"concerns\": \"Environmental impact of fishing, potential harm to marine ecosystems, lack of transparency in labeling\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Flavor Seeker Michael\",\n",
      "\"demographics\": \"28-50, foodie, enthusiastic about trying new cuisines\",\n",
      "\"motivations\": \"Wants to explore different types of fish and seafood, values unique flavors and textures\",\n",
      "\"concerns\": \"Limited access to high-quality fish, potential food safety issues, lack of knowledge about fish preparation\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Busy Parent Jennifer\",\n",
      "\"demographics\": \"30-55, suburban family, prioritizes convenience and ease of preparation\",\n",
      "\"motivations\": \"Looks for easy and convenient fish options, values pre-portioned and pre-seasoned products\",\n",
      "\"concerns\": \"Limited time for cooking, potential difficulty in getting kids to eat fish, high prices for convenience products\"\n",
      "}\n",
      "]\n",
      "[discussion_coordinator] Let's discuss the topic of space exploration. What are your thoughts on the recent advancements in private space companies like SpaceX and Blue Origin? Do you think humans will set foot on Mars in the near future?\n",
      "[discussion_coordinator] [\n",
      "{\n",
      "\"name\": \"Fish Fanatic Alex\",\n",
      "\"demographics\": \"25-45, urban dweller, interests in seafood and cooking\",\n",
      "\"motivations\": \"Seeks high-quality and fresh fish options, values the nutritional benefits of fish consumption\",\n",
      "\"concerns\": \"Limited access to fresh fish, potential mercury levels, and sustainability of fishing practices\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Eco-Warrior Maya\",\n",
      "\"demographics\": \"22-40, environmentally aware, interests in sustainable living\",\n",
      "\"motivations\": \"Seeks sustainable and responsibly sourced fish options, values eco-friendly packaging\",\n",
      "\"concerns\": \"Environmental impact of fishing practices, potential harm to marine ecosystems, and lack of transparency in labeling\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Family Foodie Tom\",\n",
      "\"demographics\": \"30-55, suburban family, prioritizes family meals and convenience\",\n",
      "\"motivations\": \"Looks for easy and convenient fish options, values pre-portioned and pre-seasoned products\",\n",
      "\"concerns\": \"Limited time for cooking, potential difficulty in getting kids to eat fish, and high prices for convenience products\"\n",
      "}\n",
      "]\n",
      "[discussion_coordinator] [\n",
      "{\n",
      "\"name\": \"Seafood Enthusiast Emily\",\n",
      "\"demographics\": \"25-45, urban dweller, interests in fine dining and seafood\",\n",
      "\"motivations\": \"Seeks high-quality and fresh fish options, values the culinary experience of seafood\",\n",
      "\"concerns\": \"Limited access to exotic or rare fish species, potential food safety issues, and high prices for premium seafood\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Health-Conscious Chris\",\n",
      "\"demographics\": \"28-50, fitness-oriented, prioritizes healthy eating\",\n",
      "\"motivations\": \"Seeks nutritious and lean fish options, values the health benefits of fish consumption\",\n",
      "\"concerns\": \"Mercury levels, potential contaminants in fish, and limited knowledge of healthy cooking methods\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Budget-Friendly Buyer Rachel\",\n",
      "\"demographics\": \"30-55, suburban family, prioritizes affordability and value\",\n",
      "\"motivations\": \"Looks for affordable and convenient fish options, values family-friendly meal solutions\",\n",
      "\"concerns\": \"Price point, potential waste and packaging concerns, and limited access to fresh seafood\"\n",
      "}\n",
      "]\n",
      "[discussion_coordinator] [\n",
      "{\n",
      "\"name\": \"Aquatic Adventurer Jack\",\n",
      "\"demographics\": \"22-40, outdoor enthusiast, interests in fishing and water sports\",\n",
      "\"motivations\": \"Seeks fresh and sustainable fish options, values the thrill of the catch and the great outdoors\",\n",
      "\"concerns\": \"Limited access to fishing spots, potential environmental impact of fishing practices, and lack of knowledge about fish species\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Foodie Fiona\",\n",
      "\"demographics\": \"25-50, urban dweller, enthusiastic about trying new cuisines\",\n",
      "\"motivations\": \"Wants to explore different types of fish and seafood, values unique flavors and textures\",\n",
      "\"concerns\": \"Limited access to exotic or rare fish species, potential food safety issues, and lack of knowledge about fish preparation\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Eco-Minded Emma\",\n",
      "\"demographics\": \"28-50, environmentally aware, prioritizes sustainable living\",\n",
      "\"motivations\": \"Seeks sustainable and responsibly sourced fish options, values eco-friendly packaging and minimal waste\",\n",
      "\"concerns\": \"Environmental impact of fishing practices, potential harm to marine ecosystems, and lack of transparency in labeling\"\n",
      "}\n",
      "]\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8f65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
