{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87dec124",
   "metadata": {},
   "source": [
    "## 搜尋網路 Function\n",
    "1. 過濾網域：先在台灣的網域 (用fstring加上`.tw`)\n",
    "\n",
    "EX:`\n",
    "search_web(\"空氣清淨機 使用心得\", site=\"ptt.cc\")\n",
    "search_web(\"Switch 遊戲 評價\", site=\"shopee.tw\")\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1da280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from googlesearch import search as _search\n",
    "from bs4 import BeautifulSoup\n",
    "from charset_normalizer import detect\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "async def worker(s:AsyncHTMLSession, url:str):\n",
    "    try:\n",
    "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
    "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
    "            return None\n",
    "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
    "        return r.text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "async def get_htmls(urls):\n",
    "    session = AsyncHTMLSession()\n",
    "    tasks = (worker(session, url) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# keyword 關鍵字 / 回覆數量 n_results (不能太高喔 => 會收到 HTTP 429 error)\n",
    "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
    "    '''\n",
    "    This function will search the keyword and return the text content in the first n_results web pages.\n",
    "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
    "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
    "    '''\n",
    "    keyword = keyword[:100]\n",
    "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
    "    # 這邊用了GoogleSearch 的 search 函式\n",
    "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
    "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
    "    results = await get_htmls(results)\n",
    "    # Filter out the None values.\n",
    "    results = [x for x in results if x is not None]\n",
    "    # Parse the HTML.\n",
    "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
    "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
    "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
    "    # Return the first n results.\n",
    "    return results[:n_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "def search_web(query, site=None, lang='zh-TW', num=100):\n",
    "    if site:\n",
    "        query = f\"{query} site:{site}\"\n",
    "    else:\n",
    "        query = f\"{query} site:.tw\"\n",
    "\n",
    "    all_results = []\n",
    "    for start in range(0, num, 10):\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"api_key\": \"YOUR_SERPAPI_KEY\",\n",
    "            \"lr\": lang,\n",
    "            \"hl\": \"zh-tw\",  # 介面語言\n",
    "            \"gl\": \"tw\",    # 地理位置\n",
    "            \"start\": start\n",
    "        }\n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()\n",
    "\n",
    "        organic = results.get(\"organic_results\", [])\n",
    "        all_results.extend([\n",
    "            {\n",
    "                \"title\": r.get(\"title\", \"\"),\n",
    "                \"link\": r.get(\"link\", \"\"),\n",
    "                \"snippet\": r.get(\"snippet\", \"\")\n",
    "            }\n",
    "            for r in organic if 'snippet' in r\n",
    "        ])\n",
    "    \n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f0bad",
   "metadata": {},
   "source": [
    "1. 用`jieba`產生中文token\n",
    "2. 移除攏言贅字 (`stopwords`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7252959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "# Optional: Define your own Chinese stopwords\n",
    "stopwords = set([\n",
    "    \"的\", \"是\", \"我\", \"也\", \"很\", \"都\", \"在\", \"有\", \"和\", \"就\", \"不\", \"了\", \"還\", \"這\", \"好\"\n",
    "])\n",
    "\n",
    "def extract_frequent_keywords(snippets, top_k=20):\n",
    "    # Combine all snippets\n",
    "    all_text = \" \".join(snippets)\n",
    "\n",
    "    # Use jieba to cut Chinese text into words\n",
    "    words = jieba.cut(all_text)\n",
    "\n",
    "    # Filter out stopwords and short tokens\n",
    "    filtered_words = [w for w in words if w not in stopwords and len(w.strip()) > 1]\n",
    "\n",
    "    # Count frequency\n",
    "    word_counts = Counter(filtered_words)\n",
    "\n",
    "    return word_counts.most_common(top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80955cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "snippets = \n",
    "top_keywords = extract_frequent_keywords(snippets)\n",
    "for word, freq in top_keywords:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
