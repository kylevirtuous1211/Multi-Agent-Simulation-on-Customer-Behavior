{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZimsCIP532LD",
        "outputId": "3e5bb08e-ed69-4dea-c98b-31426eb43e67"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HaEMYiw8Gfj",
        "outputId": "777fd59b-7bad-4806-afb5-ac08610c42d1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl (445.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m295.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.3.4\n",
            "Requirement already satisfied: googlesearch-python in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: requests-html in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.11/dist-packages (0.4.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (4.13.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (2.32.3)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.0.1)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.2.0)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.11/dist-packages (from requests-html) (1.20.2)\n",
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.3.1)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.0.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.13.2)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.4.26)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.7.0)\n",
            "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (11.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.26.20)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyquery->requests-html) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
            "Found existing installation: llama_cpp_python 0.3.4\n",
            "Uninstalling llama_cpp_python-0.3.4:\n",
            "  Successfully uninstalled llama_cpp_python-0.3.4\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m222.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m153.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m276.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m370.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m296.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m283.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for llama-cpp-python \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for llama-cpp-python\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build llama-cpp-python\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
        "else:\n",
        "    print('You are good to go!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXgs6tHQ4dBP",
        "outputId": "f83b5087-c129-43e3-858c-e3642c21e118"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are good to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 架好LLM和inference function `generate response`"
      ],
      "metadata": {
        "id": "XrpLsxpm4hwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "RItUcKrX2-zt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "# Load the model onto GPU\n",
        "\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        # stop=[\"<|eot_id|>\", \"<|end_of_text|>\"], # 這是LLama的special token, 遇到了就暫停不要繼續亂說話\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate.\n",
        "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0, # The repeat_penalty is set to 2.0. This means that the model will be strongly penalized for repeating tokens, making it more likely to generate diverse and less repetitive text.\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ],
      "metadata": {
        "id": "P2TFUZz64gVn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 網路搜尋函式，讓agent可以上網找資訊"
      ],
      "metadata": {
        "id": "yipChbe85GFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "# keyword 關鍵字 / 回覆數量 n_results (不能太高喔 => 會收到 HTTP 429 error)\n",
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:100]\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    # 這邊用了GoogleSearch 的 search 函式，回傳一堆URLs\n",
        "    urls = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    # 從URL的server去拿取HTML\n",
        "    htmls = await get_htmls(urls)\n",
        "    # 打包成 url html 的 pair\n",
        "    url_html_pairs = [(url, html) for url, html in zip(urls, htmls) if html is not None]\n",
        "    url_text_pairs = []\n",
        "    for url, html in url_html_pairs:\n",
        "      # Parse the HTML. 用 beautifulSoup將資訊解析成HTML該有的樣子(Beautiful soup object)\n",
        "        bs_object = BeautifulSoup(html, 'html.parser')\n",
        "        if detect(bs_object.encode()).get('encoding') == 'utf-8':\n",
        "\n",
        "            text_content = ''.join(bs_object.get_text().split())\n",
        "            url_text_pairs.append((url, text_content))\n",
        "\n",
        "    # Return the first n URL-result pairs\n",
        "    return url_text_pairs[:n_results]"
      ],
      "metadata": {
        "id": "2q3ZhBjn5D7Y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm_path:str):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm_path = llm_path  # LLM indicates which LLM backend this agent is using.\n",
        "        self.llm = Llama(model_path=self.llm_path, verbose=False, n_gpu_layers=-1, n_ctx=16384)\n",
        "        self.tokenizer = self.llm.tokenizer()\n",
        "\n",
        "    from llama_cpp import Llama  # Ensure this is imported\n",
        "\n",
        "    async def inference(self, message: str) -> str:\n",
        "          # Get search results\n",
        "          search_results = await search(message, n_results=3)\n",
        "\n",
        "          # Build base content without references\n",
        "          base_content = (\n",
        "              f\"Task: {self.task_description}\\n\"\n",
        "              f\"Query: {message}\\n\"\n",
        "              f\"Reference Data:\\n\"\n",
        "          )\n",
        "\n",
        "          # Calculate available tokens (16384 context - 4096 response buffer)\n",
        "          max_input_tokens = 16384 - 4096  # Adjust buffer as needed\n",
        "          used_tokens = len(self.tokenizer.encode(base_content))\n",
        "          remaining_tokens = max_input_tokens - used_tokens\n",
        "\n",
        "          # Truncate reference info\n",
        "          reference_info = []\n",
        "          if search_results:\n",
        "              for url, text in search_results:\n",
        "                  combined = f\"({url}) {text}\"\n",
        "                  tokens = self.tokenizer.encode(combined)\n",
        "                  reference_info.append(self.tokenizer.decode(tokens[:remaining_tokens//3]))  # Split tokens between 3 results\n",
        "                  remaining_tokens -= len(tokens)\n",
        "\n",
        "          # Build final messages\n",
        "          messages = [\n",
        "              {\n",
        "                  \"role\": \"system\",\n",
        "                  \"content\": self.tokenizer.decode(\n",
        "                      self.tokenizer.encode(f\"{self.role_description}\\nCurrent knowledge cutoff: May 2025\")[:512]\n",
        "                  )\n",
        "              },\n",
        "              {\n",
        "                  \"role\": \"user\",\n",
        "                  \"content\": base_content + \"\\n\".join(reference_info[:3])\n",
        "              }\n",
        "          ]\n",
        "          return generate_response(self.llm, messages)"
      ],
      "metadata": {
        "id": "mcLr6bZP8mAd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上網搜尋功能可以了"
      ],
      "metadata": {
        "id": "AN3WqRFc20qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# search_results = await search(\"名偵探柯南：貝克街的亡靈講了什麼？\", n_results = 3)\n",
        "# reference_info = \"\\n\".join(\n",
        "#     f\"({url}) {text}\" for url, text in search_results\n",
        "# ) if search_results else \"No search results found\"\n",
        "# print(reference_info)\n"
      ],
      "metadata": {
        "id": "rqQJkfARwI1V"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test\n",
        "因為Qwen-14b太大了，只要load進去GPU，RAM就佔了10.8GB，每次都要重新restart清空GPU。"
      ],
      "metadata": {
        "id": "GsV8CIxqo8CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message =\n",
        "\n",
        "role_1 = \"很會解釋電影的影評\"\n",
        "task_2 = \"用繁體中文解釋柯南電影\"\n",
        "Agent1 = LLMAgent(role, task, llm_path='/content/drive/MyDrive/Colab Notebooks/ML_models/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-IQ2_M.gguf')\n",
        "\n"
      ],
      "metadata": {
        "id": "hPrlRzyko7wr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40769b66-155d-4202-f6f9-7accab4b321a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await Agent1.inference(\"名偵探柯南：貝克街的亡靈講了什麼？\")"
      ],
      "metadata": {
        "id": "8tIHD-u5q-_n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8ff7ccd1-ed8b-4e8e-b4e2-09ed19b3d593"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'《名侦探柯南：贝克街的亡灵》是一部动画电影，讲述了两位主角——工藤新一（也就是江户川小兰）和他的朋友们在体验由“茧”构建的人造世界时遇到的事件。他们选择穿越到19世纪伦敦去破解历史上的悬案之一：“开膛手杰克”的连环杀人。这部电影以一位名叫泽田弘树的天才儿童，在相继研发出“DNA追踪系统”和 “诺亚方舟AI ”之后自殺離世为開場，讲述了柯南等人的体验过程。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "原本的:\n",
        "《名偵探柯南：貝克街的亡靈》是1986年上映的一部經典動畫電影，也是所有漫迷心中的白月光。這是一部充滿了對福爾摩斯致敬意味的作品。\\n\\n**一、故事背景與設定**\\n影片將舞台設置在20世紀初的大正時代日本和現代的東京兩大時空交織下進行探案解謎的故事，柯南一行人被卷入了一個涉及虛擬城市“米花市”的神秘案件。這個城市的建立是為了慶祝一個名為吉田鶴右衛門的人而命名。\\n\\n**二、主要情節**\\n1. **現代東京部分：尋找真相的開始與發展過程中的重重困難阻礙柯南等人解開了連環殺人案，並揭露出幕後黑手。同時還阻止了一場針對米花市數據庫的大規模破壞行動。\\n2.“大正時代”日本的部分則是通過電腦模擬的方式進行虛構的探險，在這個過程中他們遇到了許多困難和挑戰。\\n\\n**三、主題思想**\\n1. **對真相與歷史責任感：影片強調了對於過去錯誤行為應該承擔起相應的责任，並努力去修正它們。\\n2.“科技”雙刃劍的概念也得到了很好的體現。一方面它可以幫助人們解決問題；另一方面也可能會帶來新的挑戰和風險。\\n\\n**四、角色塑造**\\n1. 柯南：他展露出極高的智慧與推理能力，在整個案件中起到了關鍵作用，並且表現出了對於真相的執著追求。\\n2.“工藤新一”則是通過柯楠這個形象來進行自我成長。他在面對困難時始終保持樂觀積極的心態。\\n\\n總之，《名偵探コナン：ベイカー街の亡霊》是一部兼具娛樂性和教育意義的作品，它不僅僅是一個簡單的推理故事還讓我們看到了人性中的光明面以及對於過去錯誤行為應該承擔起相應責任的重要性。"
      ],
      "metadata": {
        "id": "IoBQ9jOJwCsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 加入web search\n",
        "《名侦探柯南：贝克街的亡灵》是20世纪初一部极具影响力的电影，它巧妙地将现实与虚拟世界结合，并通过一个引人入胜的故事探讨了社会问题和人性。这部电影不仅是一部娱乐作品,更是一次对经典文学致敬。\\n\\n**一、故事背景**\\n\\n影片以天才少年泽田弘树的自杀为开端——他因无法适应日本教育体系而前往美国，最终却在完成“诺亚方舟”人工智能后选择结束自己的生命。“茧”的虚拟现实游戏本应是场娱乐体验,但因为\"諾亞之船\"(Noah\\'s Ark)的人工智能系统入侵，“50名孩子中只要有一个通关便放过他们”，否则将用特殊电磁波杀害所有参与者。柯南一行人选择了1893年伦敦的贝克街，试图解开开膛手杰克试图谋杀艾琳·爱德华斯的历史悬案。\\n\\n**二、主要情节**\\n\\n- **现实世界**\\n  - 柯楠等人受邀参加“茧”的发布会,但游戏开始后却陷入危机。\\n  \\n    在现实中，“诺亚方舟”是泽田弘树创造的人工智能，它入侵了虚拟系统，并切断所有与外界的联系。柯南的父亲——著名侦探作家优作也出席活动并负责监修历史事件。\\n\\n- **虚幻世界**\\n  - 柯楠一行人选择1893年伦敦作为游戏场景。\\n    在这里,他们遇到了哈德森太太、莫里亚蒂教授和开膛手杰克。柯南通过推理发现，被杀害的女性是凶手的母亲，并且他因为怨恨而杀死了她。\\n\\n**三、“诺亞之船”的目的**\\n\\n“諾亜方舟”希望改变日本社会现状,避免世袭制导致的社会问题。“茧\"游戏参与者多为富二代、权贵后代。在现实世界中，他们骄纵跋扈；但在虚拟游戏中却不得不放下身段与柯南等人合作。\\n\\n**四、“诺亞之船”的启示**\\n\\n1. **教育体制的反思**\\n   - 日本现行制度无法包容像泽田弘树这样极具天赋但性格孤僻的孩子。他被老师和同学视为怪胎，最终只能选择离开日本前往美国。\\n2 . 社会阶层固化\\n    富二代们在现实世界中骄奢淫逸,但在虚拟游戏中却不得不放下身段"
      ],
      "metadata": {
        "id": "zA3jFwViAYM2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Mnhwyl-r2Dd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}